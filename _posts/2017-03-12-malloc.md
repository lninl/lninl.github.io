---
layout: post
title: "malloc"
date: 2017-03-12
---

# malloc
---
###　todo

* malloc 分配的是虚拟内存 or 物理内存？
* 虚拟内存的分配和释放的管理是 伙伴系统么？

* slab
* brk
* mmap
* malloc

![内存分配](http://i.imgur.com/SOOTIrS.png)

### malloc 
分配器本身占用空间小，分配释放效率高，减少内存碎片  
特殊的边界标记法  

为满足多线程高效分配，每个进程分配一个主分区和多个非主分区，分区之间通过循环链表链接起来。分配时线程在对应分区分配，该分区加锁时，在循环链中寻找未加锁的分区分配，没有未加锁的分区或大小不够，申请新的非主分区。非主分区只增不减。

* 主分区：可以使用sbrk、mmap分配
* 非主分区：只能 mmap 申请，一次 1MB(32)/64MB(64)

malloc 中缓存内存块，避免频繁的系统调用。ptmalloc 将相似大小的 chunk 用双向链表链接起来，一个链表被称为一个bin。ptmalloc 总共维护了 128 个 bin，并用数组存储 bin：

![malloc](http://i.imgur.com/vh6ppwX.png)  

* bin[1]：unsorted bin
* bin[2-64]：small bins：同一个 small bin 中 chunk 大小相同，相邻 small bin 中的 chunk 大小相差 8B，最后释放的 chunk 被放在链表头部，申请从链表尾部开始。
* bin[>64]：large bins：每个 bin 分别包含一个给定范围的 chunk，chunk 按大到小循序排列，形同大小按最近使用顺序排列，ptmalloc 使用“smallest-first，best-fit”原则寻找合适 chunk。

其他 chunk 组织方式

* fast bins：用于小内存空间频繁分配释放。(max_fast：64B之内)释放时不合并，申请时有限查找 fast bins。
* unsorted bin：释放大小大于 max_fast 的 chunk 先被放入该 unsorted bin 队列。申请在为找到时，将unsorted bin 中的 chunk 加入 bins 中。
* top chunk：最高地址的空间，不管大小多少，都不会被加入 bins 中，在 bins 之后被考虑
* mmaped chunk：top chunk 之后被考虑，mmap 映射
* last remainder：可分裂

分配算法概述：

* size <= 64：pool算法分配
* 64 < size < 512：最佳匹配和 pool 算法
* size >= 512：最佳匹配算法
* size >= 128KB(mmap分配阈值)：mmap 分配

> fast bins -> small bins -> unsorted bin -> large bins -> top chunk -> mmap

### slab 内核缓冲区的管理
* 面临的问题
	* 内存锁片化
	* 每次分配缓冲区需要初始化
	* 缓冲区的组织与管理
	* 多处理器共用内存
* slab 缓冲区分配和管理
	* 为各种数据结构分别建立缓冲池
	* 每种数据结构有相应的 构造/析构 函数，用于初始化和回复原状
	* 队列动态变化，少补多减
	* 缓冲区队列由一连串的 slab 构成，slab 中包含若干同种的对象
* slab 缓冲区
	* 专用缓冲区队列，层次式
		* 总根 cache_cache，维持第一层 slab 队列
		* 第一层 slab 对象，kmem_cache_t，维持第二层 slab 队列
		* 第二层 slab 队列为某种数据结构专用
		* 每个第二层 slab 上都维持一个空闲对象队列
	* 通用缓冲池 slab_cash
		* 顶层时结构数组，数组的每个元素指向不同的slab队列
		* 数组所载对象大小 32,64,128...,128k(32 page)
		* kmalloc()、 kfree()

### brk() 系统调用 ？只申请虚拟空间，未建立页面表项的映射？
* 用户进程向内核申请空间(brk批发，malloc零售)
* 每个进程拥有3G的虚拟空间，虚拟空间最终映射到某个物理存储空间才可以使用，这种映射的建立和管理由内核处理。采用按需分配
* 内核如何管理每个进程的3G虚拟空间？
	* 管理的空间范围：数据段的顶部 end_data 到堆栈段的下沿
	* 从 end_data 边界开始，向上推进
	* 内核将当前的边界记录在进程的 mm_struct 结构中的 brk
	* 进程需要分配内存时，将要求的大小与当前的动态分配区底部相加
	* 边界必须与页面大小对齐
* 释放空间 (新边界低于老边界)
	* 解除虚存空间的页面映射
		* 在页面目录中找到起始地址所属的目录项
		* 将解除映射的页面表项清成0
		* 解除对内存页面以及盘上页面的使用
	* 解除虚拟空间，释放物理页面
		* 用户空间映射、可换出的内存页面同时存在三个队列中
			* 换入/换出队列
			* LRU 队列
			* 哈希队列
		* 把页面的内容"冲刷"到块设备上
		* 将页面从所在的队列中脱离出来
		* 调整虚存区间和进程的数据结构
		* 释放空白的页面表
* 分配空间 (新边界高于老边界)
	* 检查对进程的资源限制，及是否与已存在的区间冲突(高端、低端)
	* 查询系统中是否有足够的空闲内存页面
	* 建立新的映射
		* 判断是否可以与原有的区间合并，通过扩展原有区间覆盖新增的区间

* 页面表项的映射是否建立？
* 从新分配的区间中读，内容是什么？
### 8 mmap() 系统调用
* 将一个已经打开文件的内容映射到它的用户空间
* 映射地址知识一个参考值，不能满足时可以由内核分配一个
* 内核从 (TASK_SIZE/3) 即 1GB 处开始向上在当前进程的虚存空间中寻找一块足以容纳给定长度的区间
* file 结构指针为 0 ，目的仅在与创建虚存区间
* 文件与虚存区间之间的映射包含两个环节：
	* 物理页面与文件映象之间的换入/换出
	* 物理页面与虚存页面之间的映射
* **mmap内存映射的过程：**
	* 在进程的虚拟地址空间中创建虚拟映射区域
		* 在当前进程的虚拟地址空间中，寻找一段空闲的满足要求的连续的虚拟地址
		* 为此虚拟区分配一个vm_area_struct结构，接着对这个结构的各个域进行了初始化
		* 将新建的虚拟区结构（vm_area_struct）插入进程的虚拟地址区域链表或树中
	* 建立文件物理地址和进程虚拟地址的映射关系
		* 为映射分配了新的虚拟地址区域后，通过待映射的文件指针，在文件描述符表中找到对应的文件描述符，通过文件描述符，链接到内核“已打开文件集”中该文件的文件结构体，每个文件结构体维护着和这个已打开文件相关各项信息。
		* 内核mmap函数通过虚拟文件系统inode模块定位到文件磁盘物理地址。
		* 通过remap_pfn_range函数建立页表，即实现了文件地址和虚拟地址区域的映射关系。此时，这片虚拟地址并没有任何数据关联到主存中。
		
>具体的页面映射推迟到真正需要的是够才进行；
>
>重要的不是建立一个特定的映射，而是建立一套机制，使得一旦需要时就可以根据当时的具体情况建立起新的映射
>
>lazy computation 有些为将来做某种准备而进行的操作可能并无必要，应该推迟到真正需要时才进行。

* 与 brk 的区别
	* do_brk() 区间可能合并，不用分配 vm_area_struct数据结构
	* mmap() 却是必须分配，属性不同的区段不能共存于同一个逻辑区间中，映射到特定的文件也是一种属性，所以单独建立逻辑区间

### 参考文献

[内存分配 kmalloc vmalloc malloc](http://blog.leanote.com/post/804305986@qq.com/%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D)  
[glibc内存分配基础](http://www.hulkdev.com/posts/glibc-basic)
